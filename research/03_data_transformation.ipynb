{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\college_work\\\\4th year\\\\Sem7th\\\\Project\\\\OBE\\\\Paper- Blooms taxanomy\\\\Boolm-s-Level-Detection-A-MLOPS-Project'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\college_work\\\\4th year\\\\Sem7th\\\\Project\\\\OBE\\\\Paper- Blooms taxanomy\\\\Boolm-s-Level-Detection-A-MLOPS-Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotlib.inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib.inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./artifacts/data_ingestion/blooms_dataset.csv\")\n",
    "print(df.info())\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Level\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\" {df[df['Text'].str.startswith(' ')].shape}\")\n",
    "# print(f\" {df[df['Text'].str.endswith(' ')].shape}\")\n",
    "\n",
    "# df['Text'] = df['Text'].str.strip()\n",
    "\n",
    "# print(f\" {df[df['Text'].str.startswith(' ')].shape}\")\n",
    "# print(f\" {df[df['Text'].str.endswith(' ')].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating another dataframe with lower cases \n",
    "because uncased data is only used with pretrained model like BERD because they are case sensetive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is lower (600, 2)\n",
      "is lower (600, 2)\n"
     ]
    }
   ],
   "source": [
    "bert_df = df\n",
    "print(f\"is lower {df[df['Text'].str.islower()].shape}\")\n",
    "df[\"Text\"] = df[\"Text\"].str.lower()\n",
    "print(f\"is lower {df[df['Text'].str.islower()].shape}\")\n",
    "# removing some extra space\n",
    "df[\"Text\"] = df[\"Text\"].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "augmented each Text input 3 times with following parameter with nlpaug library\n",
    "\n",
    "```ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"insert\")```\n",
    "\n",
    "before augmendatation data shape : 600,2\n",
    "total 6 unique calsses 100 each \n",
    "\n",
    "augmented data shape : 1800,2\n",
    "total 6 unique classes 300 each\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def augment_text(text):\n",
    "    aug = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"insert\")\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "\n",
    "def augment_rows_pytorch(row):\n",
    "    augmented_texts = [augment_text(row[\"Text\"]) for _ in range(3)]\n",
    "    text_list = [text for text in augmented_texts]\n",
    "    level_list = [row[\"Level\"]] * 3\n",
    "    return {\"Text\": text_list, \"Level\": level_list}\n",
    "\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Augmenting Text\"):\n",
    "    augmented_data.append(augment_rows_pytorch(row))\n",
    "\n",
    "augmented_data_flat = [\n",
    "    {\"Text\": Text, \"Level\": Level}\n",
    "    for row in augmented_data\n",
    "    for Text, Level in zip(row[\"Text\"], row[\"Level\"])\n",
    "]\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data_flat)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nAugmented DataFrame:\")\n",
    "print(augmented_df.head())\n",
    "\n",
    "print(\"\\nFinal DataFrame Shape:\", augmented_df.shape)\n",
    "\n",
    "\n",
    "augmented_df.to_csv(\"./artifacts/data_transformation/augmented_bloom_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1800 entries, 0 to 1799\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    1800 non-null   object\n",
      " 1   Level   1800 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 42.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['is about average what proportion part of in ...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['about seeing what huge proportion of society...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['how about what proportion of that the popula...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['correctly label the three brain lobes are in...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['most correctly correct label the brain lobes...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['correctly label the two brain lobes indicate...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['and define compound by interest.']</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['this define of compound interest.']</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['and define their compound interest.']</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['define those four types part of data traceab...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Level\n",
       "0  ['is about average what proportion part of in ...  Remember\n",
       "1  ['about seeing what huge proportion of society...  Remember\n",
       "2  ['how about what proportion of that the popula...  Remember\n",
       "3  ['correctly label the three brain lobes are in...  Remember\n",
       "4  ['most correctly correct label the brain lobes...  Remember\n",
       "5  ['correctly label the two brain lobes indicate...  Remember\n",
       "6               ['and define compound by interest.']  Remember\n",
       "7              ['this define of compound interest.']  Remember\n",
       "8            ['and define their compound interest.']  Remember\n",
       "9  ['define those four types part of data traceab...  Remember"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the augmented data \n",
    "df = pd.read_csv('./artifacts/data_transformation/augmented_bloom_data.csv',index_col=0)\n",
    "print(df.info())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is about average what proportion part of in th...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>about seeing what huge proportion of society t...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how about what proportion of that the populati...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>correctly label the three brain lobes are indi...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>most correctly correct label the brain lobes i...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>correctly label the two brain lobes indicated ...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and define compound by interest.</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this define of compound interest.</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and define their compound interest.</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>define those four types part of data traceabil...</td>\n",
       "      <td>Remember</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text     Level\n",
       "0  is about average what proportion part of in th...  Remember\n",
       "1  about seeing what huge proportion of society t...  Remember\n",
       "2  how about what proportion of that the populati...  Remember\n",
       "3  correctly label the three brain lobes are indi...  Remember\n",
       "4  most correctly correct label the brain lobes i...  Remember\n",
       "5  correctly label the two brain lobes indicated ...  Remember\n",
       "6                   and define compound by interest.  Remember\n",
       "7                  this define of compound interest.  Remember\n",
       "8                and define their compound interest.  Remember\n",
       "9  define those four types part of data traceabil...  Remember"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the square bracket of each text \n",
    "def remove_bracket(text):\n",
    "    return text[2:-2]\n",
    "\n",
    "text = \"[/dlfjasldfjasldfjlsdfj]\"\n",
    "df['Text'] = df['Text'].apply(lambda x: remove_bracket(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Performing the nltk and berttokenization to show the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1622, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df['Text'].str.contains(r'\\.{2,}',regex=True)].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# test = \"this is an example of ANN\"\n",
    "\n",
    "# tokens = tokenizer(test,return_tensors='pt')\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [is, about, average, what, proportion, part, o...\n",
       "1    [about, seeing, what, huge, proportion, of, so...\n",
       "2    [how, about, what, proportion, of, that, the, ...\n",
       "3    [correctly, label, the, three, brain, lobes, a...\n",
       "4    [most, correctly, correct, label, the, brain, ...\n",
       "5    [correctly, label, the, two, brain, lobes, ind...\n",
       "6             [and, define, compound, by, interest, .]\n",
       "7            [this, define, of, compound, interest, .]\n",
       "8          [and, define, their, compound, interest, .]\n",
       "9    [define, those, four, types, part, of, data, t...\n",
       "Name: Tokens, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df[\"Tokens\"] = [word_tokenize(text) for text in df[\"Text\"]]\n",
    "df['Tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./artifacts/data_transformation/clean_blooms_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLops steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    clean_data_path: Path\n",
    "    augmented_data_path: Path\n",
    "    new_data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blooms.constants import *\n",
    "from blooms.utils.common import read_yaml, create_directories\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_tranformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            clean_data_path=config.clean_data_path,\n",
    "            augmented_data_path=config.augmented_data_path,\n",
    "            new_data_path=config.new_data_path,\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from blooms import logger\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DataTrnsformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def data_preprocessing(self):\n",
    "        df = pd.read_csv(self.config.data_path, index_col=None)\n",
    "        df[\"Text\"] = df[\"Text\"].str.strip()\n",
    "        df[\"Text\"] = df[\"Text\"].str.lower()\n",
    "        df.to_csv(self.config.clean_data_path, index=False)\n",
    "\n",
    "    def augment_text(text):\n",
    "        \"\"\"\n",
    "        augmented each Text input 3 times with following parameter with nlpaug library\n",
    "\n",
    "        ```ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"insert\")```\n",
    "\n",
    "        before augmendatation data shape : 600,2\n",
    "        total 6 unique calsses 100 each \n",
    "\n",
    "        augmented data shape : 1800,2\n",
    "        total 6 unique classes 300 each\n",
    "        \"\"\"\n",
    "        aug = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"insert\")\n",
    "        augmented_text = aug.augment(text)\n",
    "        return augmented_text\n",
    "\n",
    "    def augment_rows(row):\n",
    "        augmented_texts = [augment_text(row[\"Text\"]) for _ in range(3)]\n",
    "        text_list = [text for text in augmented_texts]\n",
    "        level_list = [row[\"Level\"]] * 3\n",
    "        return {\"Text\": text_list, \"Level\": level_list}\n",
    "\n",
    "    def data_augmentation(self):\n",
    "        df = pd.read_csv(self.config.clean_data_path, index_col=None)\n",
    "        augmented_data = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Augmenting Text\"):\n",
    "            augmented_data.append(augment_rows(row))\n",
    "        augmented_data_flat = [\n",
    "            {\"Text\": Text, \"Level\": Level}\n",
    "            for row in augmented_data\n",
    "            for Text, Level in zip(row[\"Text\"], row[\"Level\"])\n",
    "        ]\n",
    "        augmented_df = pd.DataFrame(augmented_data_flat)\n",
    "        augmented_df.to_csv(self.config.augmented_data_path)\n",
    "        logger.info(\n",
    "            f\"Done the augmentation! Saving the augmented data file at: {self.config.augmented_data_path}\"\n",
    "        )\n",
    "\n",
    "    def tokenization(self):\n",
    "        df = pd.read_csv(self.config.augmented_data_path, index_col=0)\n",
    "        df['Text'] = df['Text'].apply(lambda x: x[2:-2])\n",
    "        df = df.drop(df[df['Text'].str.contains(r'\\.{2,}',regex=True)].index)\n",
    "        df[\"Tokens\"] = [word_tokenize(text) for text in df[\"Text\"]]\n",
    "        df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "        \n",
    "        df.to_csv(self.config.new_data_path, index=False)\n",
    "        logger.info(\n",
    "            f\"Done the tokenization! Saving the final cleaned file at: {self.config.new_data_path}\"\n",
    "        )\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        data = pd.read_csv(self.config.new_data_path)\n",
    "        train, test = train_test_split(data)\n",
    "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\n",
    "        logger.info(\n",
    "            f\"Done the Splitting! Saving the train and test  file at: {self.config.root_dir}\"\n",
    "        )\n",
    "        logger.info(f\"Splited data into training and testing sets\")\n",
    "        logger.info(train.shape)\n",
    "        logger.info(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: common: yaml file: config\\config.yaml loaded succcessfully]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: common: yaml file: params.yaml loaded succcessfully]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: common: yaml file: schema.yaml loaded succcessfully]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: common: created directory at : artifacts]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: common: created directory at : artifacts/data_transformation]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: 3519884042: Done the tokenization! Saving the final cleaned file at: artifacts/data_transformation/new_blooms_dataset.csv]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: 3519884042: Done the Splitting! Saving the train and test  file at: artifacts/data_transformation]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: 3519884042: Splited data into training and testing sets]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: 3519884042: (1216, 3)]\u001b[0m\n",
      "[2023-12-18 21:39:06: \u001b[32mINFO\u001b[0m: 3519884042: (406, 3)]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transfermation_config = config.get_data_tranformation_config()\n",
    "    data_transfermation = DataTrnsformation(config=data_transfermation_config)\n",
    "    data_transfermation.data_preprocessing()\n",
    "    # data_transfermation.data_augmentation()\n",
    "    data_transfermation.tokenization()\n",
    "    data_transfermation.train_test_splitting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blooms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
