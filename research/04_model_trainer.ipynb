{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\college_work\\\\4th year\\\\Sem7th\\\\Project\\\\OBE\\\\Paper- Blooms taxanomy\\\\Boolm-s-Level-Detection-A-MLOPS-Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\college_work\\\\4th year\\\\Sem7th\\\\Project\\\\OBE\\\\Paper- Blooms taxanomy\\\\Boolm-s-Level-Detection-A-MLOPS-Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing and researching for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./artifacts/data_transformation/train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 NB claassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9261083743842364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_data = pd.read_csv(\"./artifacts/data_transformation/train.csv\")\n",
    "test_data = pd.read_csv(\"./artifacts/data_transformation/test.csv\")\n",
    "\n",
    "train_x = train_data[\"Text\"]\n",
    "train_y = train_data[\"Level\"]\n",
    "test_x = test_data[\"Text\"]\n",
    "test_y = test_data[\"Level\"]\n",
    "\n",
    "model_Nb = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer()),\n",
    "        (\"SMOTE \", SMOTE(random_state=12)),\n",
    "        (\"MultinomialNB\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_Nb.fit(train_x, train_y)\n",
    "model_Nb_predit = model_Nb.predict(test_x)\n",
    "# \n",
    "# print(model_Nb_predit)\n",
    "\n",
    "# print(classification_report(test_y, model_Nb_predit))\n",
    "print(f\"accuracy {accuracy_score(test_y,model_Nb_predit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Remember'], dtype='<U10')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Nb_predit = model_Nb.predict([\"What are our organizationâ€™s core values?\"])\n",
    "model_Nb_predit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2 distil bert classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow==2.15.0\n",
    "%pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://ukantjadia.me/bt/train.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sns.displot(df['count'])\n",
    "plt.xlabel('The no of words')\n",
    "plt.ylabel('no of words distribution ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = df['Level'].value_counts()\n",
    "categories = category_count.index\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (12, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.barplot(x = category_count.index, y = category_count )\n",
    "for a, p in enumerate(ax.patches):\n",
    "    ax.annotate(f'{categories[a]}\\n' + format(p.get_height(), '.0f'), xy = (p.get_x() + p.get_width() / 2.0, p.get_height()), xytext = (0,-25), size = 13, color = 'white' , ha = 'center', va = 'center', textcoords = 'offset points', bbox = dict(boxstyle = 'round', facecolor='none',edgecolor='white', alpha = 0.5) )\n",
    "\n",
    "plt.xlabel('Categories', size = 15)\n",
    "plt.ylabel('The Number of News', size= 15)\n",
    "plt.xticks(size = 12)\n",
    "plt.title(\"The number of News by Categories\" , size = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoded_text'] = df['Level'].astype('category').cat.codes\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts = df['Text'].to_list()\n",
    "data_labels = df['encoded_text'].to_list()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels,test_size = 0.2,random_state = 0)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels,test_size = 0.01,random_state = 0)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)\n",
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=1e-5,\n",
    "    logging_dir='./logs',\n",
    "    eval_steps=100\n",
    ")\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    trainer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels = 6)\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model = trainer_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"saved_models\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fine_tuned = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "model_fine_tuned = TFDistilBertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"How wdfsdfsf this type of emergency?\"\n",
    "predict_input = tokenizer_fine_tuned.encode(\n",
    "    test_text,\n",
    "    truncation = True,\n",
    "    padding = True,\n",
    "    return_tensors = 'tf'\n",
    ")\n",
    "\n",
    "output = model_fine_tuned(predict_input)[0]\n",
    "\n",
    "prediction_value = tf.argmax(output, axis = 1).numpy()[0]\n",
    "\n",
    "prediction_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 3 word to vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word_vect = api.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "!python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenises a sentence using spacy library\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    # lemmatise tokens\n",
    "    tokens = [word.lemma_.strip() for word in doc]\n",
    "    return tokens\n",
    "\n",
    "def sentence_vect(tokens):\n",
    "    size = word_vect.vector_size\n",
    "    sent_vect = np.zeros(size)\n",
    "    counter = 0\n",
    "    for word in tokens:\n",
    "        if word in word_vect:\n",
    "            sent_vect += word_vect[word]\n",
    "            counter += 1\n",
    "\n",
    "    if counter == 0:\n",
    "        return None\n",
    "    sent_vect = sent_vect / counter\n",
    "    return sent_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "from itertools import islice\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = f\"https://ukantjadia.me/bt/train.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df['new_Tokens'] = df['Text'].apply(spacy_tokenizer)\n",
    "# create sentence vectors by taking average of token vectors\n",
    "df['Vectors'] = df['new_Tokens'].apply(sentence_vect)\n",
    "df = df.dropna(axis=0, subset=['Vectors'])\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='linear', gamma='auto', degree=3)\n",
    "categories = ['Remember', 'Understand', 'Apply', 'Analyse', 'Evaluate', 'Create']\n",
    "model.fit(list(train.Vectors), train.Level)\n",
    "labels = model.predict(list(test.Vectors))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "mat = confusion_matrix(test.Level, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False\n",
    "            , xticklabels=categories\n",
    "            , yticklabels=categories)\n",
    "plt.xlabel('true Level')\n",
    "plt.ylabel('predicted Level')\n",
    "\n",
    "print(classification_report(test.Level, labels))\n",
    "acc = accuracy_score(test.Level, labels)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {0: 'Remember', 1: 'Understand', 2: 'Apply', 3: 'Analyse', 4: 'Evaluate', 5: 'Crexcate'}\n",
    "\n",
    "def predict_blooms(text, model):\n",
    "    process = spacy_tokenizer(text)\n",
    "    processes = sentence_vect(process)\n",
    "    blooms = model.predict([processes])\n",
    "    print(\"Predicted Class:\", blooms)\n",
    "    \n",
    "print(categories, \"\\n\")\n",
    "\n",
    "task = \"What changes would you make to this example contract?\"\n",
    "\n",
    "print(\"\\nSVM PREDICTION:\")\n",
    "predict_blooms(task, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    target_columns: str\n",
    "    training_columns: str\n",
    "    pipeline_ele_1: str\n",
    "    pipeline_ele_2: str\n",
    "    pipeline_ele_3: str\n",
    "    elastic_model_name: str\n",
    "    multinomial_model_name: str\n",
    "    word2vector_model_name: str\n",
    "    distilBERT_model_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blooms.constants import *\n",
    "from blooms.utils.common import read_yaml, create_directories\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        params = self.params.Models_03\n",
    "        schema = self.schema.COLUMNS\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            target_columns=schema.target_column,\n",
    "            training_columns=schema.training_columns,\n",
    "            elastic_model_name=config.elastic_model_name,\n",
    "            multinomial_model_name=config.multinomial_model_name,\n",
    "            distilBERT_model_name=config.distilBERT_model_name,\n",
    "            word2vector_model_name=config.word2vector_model_name,\n",
    "            pipeline_ele_1=params.pipeline_ele_1,\n",
    "            pipeline_ele_2=params.pipeline_ele_2,\n",
    "            pipeline_ele_3=params.pipeline_ele_3,\n",
    "        )\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blooms import logger\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        train_x = train_data[self.config.training_columns]\n",
    "        train_y = train_data[[self.config.target_columns]]\n",
    "\n",
    "        print(f\"this is the train x size {train_x.head(2)}\")\n",
    "        print(f\"this is the train y size {train_y.head(2)}\")\n",
    "        # elasticNet = ElasticNet(\n",
    "        #     alpha=self.config.alpha, l1_ratio=self.config.l1_ratio, random_state=34\n",
    "        # )\n",
    "        # elasticNet.fit(train_x, train_y)\n",
    "        # joblib.dump(elasticNet, os.path.join(self.config.root_dir, self.config.el))\n",
    "\n",
    "        multiNomial = Pipeline(\n",
    "            [\n",
    "                (self.config.pipeline_ele_1, TfidfVectorizer()),\n",
    "                (self.config.pipeline_ele_2, SMOTE(random_state=12)),\n",
    "                (self.config.pipeline_ele_3, MultinomialNB()),\n",
    "            ]\n",
    "        )\n",
    "        multiNomial.fit(train_x, train_y)\n",
    "        joblib.dump(\n",
    "            multiNomial,\n",
    "            os.path.join(self.config.root_dir, self.config.multinomial_model_name),\n",
    "        )\n",
    "        logger.info(f\"Model training done! Model saved at {self.config.multinomial_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-19 11:01:45: \u001b[32mINFO\u001b[0m: common: yaml file: config\\config.yaml loaded succcessfully]\u001b[0m\n",
      "[2023-12-19 11:01:45: \u001b[32mINFO\u001b[0m: common: yaml file: params.yaml loaded succcessfully]\u001b[0m\n",
      "[2023-12-19 11:01:45: \u001b[32mINFO\u001b[0m: common: yaml file: schema.yaml loaded succcessfully]\u001b[0m\n",
      "[2023-12-19 11:01:45: \u001b[32mINFO\u001b[0m: common: created directory at : artifacts]\u001b[0m\n",
      "[2023-12-19 11:01:45: \u001b[32mINFO\u001b[0m: common: created directory at : artifacts/model_training]\u001b[0m\n",
      "this is the train x size 0    list are four special types of personal clause...\n",
      "1    prepare a special book jacket binding that acc...\n",
      "Name: Text, dtype: object\n",
      "this is the train y size       Level\n",
      "0  Remember\n",
      "1    Create\n",
      "[2023-12-19 11:01:46: \u001b[32mINFO\u001b[0m: 3807993266: Model training done! Model saved at multinomial_model.joblib]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ukant\\anaconda3\\envs\\blooms\\Lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_training_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blooms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
